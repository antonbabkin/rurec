# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/nbs/infogroup.ipynb (unless otherwise specified).

__all__ = ['data_years', 'convert_schema', 'get_schema', 'pad_with_zeroes', 'validate_raw_strings', 'convert_dtypes',
           'validate_raw_numbers', 'replace_invalid', 'get_df', 'describe_variable', 'style']

# Cell
import sys
import logging
import json
import gzip
import shutil

import numpy as np
import pandas as pd
import fastparquet
from IPython import display

from rurec import resources
from .resources import Resource, cacheable
from rurec import util

data_years = range(1997, 2018)

# Cell

def convert_schema(datapackage_schema_path):
    """Convert old datapackage schema.json into a new file, to be used for data validation."""
    sch0 = json.load(open(datapackage_schema_path))

    def get_field_years(field_name):
        years = []
        for fl in sch0['field_lists']:
            if field_name in fl['fields']:
                years += fl['years']
        if 2015 in years:
            years += [2016, 2017]
        if field_name == 'gender':
            years += [2017]
        return sorted(years)


    sch = dict()
    sch['info'] = 'Schema for cleaned InfoGroup data.'
    sch['fields'] = list()

    for f0 in sch0['fields']:
        f = dict()
        name = f0['name']
        f['name'] = name.upper()
        f['years'] = get_field_years(name)
        if 'enum' in f0['constraints']:
            enum = f0['constraints']['enum'].copy()
            if '' in enum:
                enum.pop(enum.index(''))
            f['enum'] = enum
        if 'values' in f0:
            values = f0['values'].copy()
            if name == 'cbsa_level':
                del values['0'] # code never used
            f['enum_labels'] = values

        field_widths = {
            'ZIP': 5,
            'ZIP4': 4,
            'COUNTY_CODE': 3,
            'AREA_CODE': 3,
            'PHONE': 7,
            'SIC': 6,
            'SIC0': 6,
            'SIC1': 6,
            'SIC2': 6,
            'SIC3': 6,
            'SIC4': 6,
            'NAICS': 8,
            'YP_CODE': 5,
            'ABI': 9,
            'SUBSIDIARY_NUMBER': 9,
            'PARENT_NUMBER': 9,
            'SITE_NUMBER': 9,
            'CENSUS_TRACT': 6,
            'CENSUS_BLOCK': 1,
            'CBSA_CODE': 5,
            'CSA_CODE': 3,
            'FIPS_CODE': 5
        }
        if f['name'] in field_widths:
            f['width'] = field_widths[f['name']]

        f['original_name'] = f0['originalName']
        f['original_description'] = f0['originalDescription']
        sch['fields'].append(f)

    json.dump(sch, open(resources.get('infogroup/schema').path, 'w'), indent=1)


def get_schema(field_name=None, year=None):
    """Flexible function to get dataset schema.

    With no parameters, return full list of field dictionaries.
    With `year`, restrict to fields present in that year.
    With `field_name`, return dictionary for specific field (`year` is ignored in this case).
    """
    sch = json.load(resources.get(f'infogroup/schema').path.open())['fields']
    if field_name is not None:
        return [x for x in sch if x['name'] == field_name][0]
    if year is not None:
        return [x for x in sch if year in x['years']]
    return sch


def pad_with_zeroes(df, schema_fields):
    """Prepend string column values with zeroes to have constant width."""

    for field in schema_fields:
        if 'width' in field:
            df[field['name']] = df[field['name']].str.zfill(field['width'])

def validate_raw_strings(df, schema_fields):
    """Validate values in raw InfoGroup data according to string constraints.
    Return list of dicts of invalid values.
    """

    constraints = {
        'ZIP': {'number': True},
        'ZIP4': {'number': True},
        'COUNTY_CODE': {'number': True},
        'AREA_CODE': {'number': True},
        'PHONE': {'number': True},
        'SIC': {'number': True},
        'SIC0': {'number': True},
        'SIC1': {'number': True},
        'SIC2': {'number': True},
        'SIC3': {'number': True},
        'SIC4': {'number': True},
        'NAICS': {'number': True},
        'YEAR': {'notna': True, 'number': True},
        'YP_CODE': {'number': True},
        'EMPLOYEES': {'number': True},
        'SALES': {'number': True},
        'PARENT_EMPLOYEES': {'number': True},
        'PARENT_SALES': {'number': True},
        'YEAR_EST': {'number': True},
        'ABI': {'unique': True, 'notna': True, 'number': True},
        'SUBSIDIARY_NUMBER': {'number': True},
        'PARENT_NUMBER': {'number': True},
        'SITE_NUMBER': {'number': True},
        'CENSUS_TRACT': {'number': True},
        'CENSUS_BLOCK': {'number': True},
        'LATITUDE': {'number': True},
        'LONGITUDE': {'number': True},
        'CBSA_CODE': {'number': True},
        'CSA_CODE': {'number': True},
        'FIPS_CODE': {'number': True}
    }

    # the above hard coded list of constraints must be consistent with field availability in given year
    constraints = {k: v for k, v in constraints.items() if k in df}


    for field in schema_fields:
        name = field['name']
        if 'enum' in field:
            if name not in constraints: constraints[name] = dict()
            constraints[name]['cats'] = field['enum']
        if 'width' in field:
            constraints[name]['nchar'] = field['width']

    return util.validate_values(df, constraints)


def convert_dtypes(df, schema_fields):
    """Inplace convert string columns to appropriate types."""

    for col in ['YEAR', 'EMPLOYEES', 'SALES', 'PARENT_EMPLOYEES', 'PARENT_SALES', 'YEAR_EST', 'LATITUDE', 'LONGITUDE']:
        df[col] = pd.to_numeric(df[col])

    for field in schema_fields:
        if 'enum' in field:
            df[field['name']] = pd.Categorical(df[field['name']], categories=field['enum'])


def validate_raw_numbers(df):
    """Validate values in raw InfoGroup data according to numerical constraints.
    Return list of dicts of invalid values.
    """

    constraints = {
        'YEAR': {'eq': year},
        'EMPLOYEES': {'ge': 0},
        'SALES': {'ge': 0},
        'PARENT_EMPLOYEES': {'ge': 0},
        'PARENT_SALES': {'ge': 0},
        'YEAR_EST': {'ge': 1000, 'le': year},
        'LATITUDE': {'ge': 0, 'le': 90},
        'LONGITUDE': {'ge': -180, 'le': 0}
    }
    return util.validate_values(df, constraints)

def replace_invalid(df, invalid_list, replacement=np.nan):
    """Replace invalid values."""
    for inv in invalid_list:
        df.loc[inv['idx'], inv['col']] = replacement
        logging.info(f'Replace invalid value `{inv["val"]}` with `{replacement}` at .loc[{inv["idx"]}, \'{inv["col"]}\']')

# Cell
def get_df(years=None, cols=None, states=None):
    """Return one year of InfoGroup data with appropriate data types.
    Subset of columns can be loaded by passing list to `cols`.
    """
    filters = []
    if years is not None:
        filters.append(('YEAR', 'in', years))
    if states is not None:
        filters.append(('STATE', 'in', states))
    res = resources.get('infogroup/full')
    df = pd.read_parquet(res.path, 'fastparquet', columns=cols, filters=filters)
    df['YEAR'] = df['YEAR'].astype(int)
    return df

# Cell
@cacheable
def describe_variable(col, df=None, distribution=False, count_values=[]):
    """Return summary statistics of `col` for all years of data.

    Dataframe is read from disk unless given in `df`.
    Distribution moments are reported by setting `distribution` to True.
    Counts of values specified in `count_values` are reported.
    """

    if df is None:
        df = get_df(cols=['YEAR', col])

    stats = {}
    stats['Total'] = df.groupby('YEAR').size()
    stats['Total']['All'] = len(df)
    df['__FLAG'] = df[col].notna()
    stats['Not missing'] = df.groupby('YEAR')['__FLAG'].sum().astype(int)
    stats['Not missing']['All'] = stats['Not missing'].sum()
    stats['Missing'] = stats['Total'] - stats['Not missing']
    stats['Unique'] = df.groupby('YEAR')[col].nunique()
    stats['Unique']['All'] = df[col].nunique()

    if distribution:
        stats['Min'] = df.groupby('YEAR')[col].min()
        stats['Min']['All'] = stats['Min'].min()
        stats['Max'] = df.groupby('YEAR')[col].max()
        stats['Max']['All'] = stats['Max'].max()
        stats['Mean'] = df.groupby('YEAR')[col].mean()
        stats['Mean']['All'] = df[col].mean()
        stats['s.d.'] = df.groupby('YEAR')[col].std()
        stats['s.d.']['All'] = df[col].std()
        q = df.groupby('YEAR')[col].quantile([0.25, 0.5, 0.75]).unstack()
        qa = df[col].quantile([0.25, 0.5, 0.75])
        stats['25%'] = q[0.25]
        stats['25%']['All'] = qa[0.25]
        stats['50% (median)'] = q[0.5]
        stats['50% (median)']['All'] = qa[0.5]
        stats['75%'] = q[0.75]
        stats['75%']['All'] = qa[0.75]

    for val in count_values:
        df['__FLAG'] = (df[col] == val)
        stats[f'Value "{val}"'] = df.groupby('YEAR')['__FLAG'].sum().astype(int)
        stats[f'Value "{val}"']['All'] = stats[f'Value "{val}"'].sum()

    return pd.concat(stats, 1)


def style(df):
    """Apply formatting style to summary stats dataframe."""

    f = {}
    int_cols = ['Total', 'Not missing', 'Missing', 'Unique']
    int_cols += [c for c in df if c.startswith('Value "')]
    for c in int_cols:
        f[c] = '{:,}'
    for c in ['Min', 'Max', 'Mean', 's.d.', '25%', '50% (median)', '75%']:
        f[c] = '{:,g}'

    return df.style.format(f)
