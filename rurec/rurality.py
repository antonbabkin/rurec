# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/rurality.ipynb (unless otherwise specified).

__all__ = ['memory', 'prepare_outside_ua_df', 'nearest_gridpoint', 'prepare_ui_df', 'prepare_ruc_df', 'prepare_ruca_df',
           'gen_rural_hrsa', 'get_hrsa_rural_in_cbsa', 'get_far_df', 'add_rural_cols', 'build_parquet_dataset',
           'get_df']

# Cell

import os
import sys
import logging
import time
import shutil
import multiprocessing as mp

import numpy as np
import pandas as pd
import geopandas as gpd
from joblib import Memory
import fastparquet

from rurec import infogroup, ers_codes, resources
from .resources import Resource

memory = Memory(resources.paths.cache)

# Cell
# for batch runs: log to a file
logging.basicConfig(filename=resources.paths.root / 'logs/build_rural.log',
                    force=True, filemode='w', level=logging.INFO,
                    format='%(asctime)s %(levelname)s:\n%(message)s')

# Cell

@memory.cache
def prepare_outside_ua_df():
    df = pd.read_csv('/InfoGroup/data/rurality/reference/geographical/points-in-polygons/data/all_tracts.csv',
                     usecols=['GEOID', 'UA_GEOID10', 'UATYP10', 'rural_tract'], dtype=object)
    df.rename(columns={'GEOID': 'CENSUS_TRACT_FULL', 'UA_GEOID10': 'UA_CODE',
                       'UATYP10': 'UA_TYPE', 'rural_tract': 'RURAL_OUTSIDE_UA'}, inplace=True)
    return df


# Cell

def nearest_gridpoint(x, grid):
    """Return value from `grid` that is nearest to `x`."""
    grid = np.sort(grid)
    ids = np.arange(len(grid))
    i = np.interp(x, grid, ids)
    i = round(i)
    return grid[i]

# Cell

@memory.cache
def prepare_ui_df(year):
    df = ers_codes.get_ui_df()[['FIPS', 'UI_YEAR', 'UI_CODE']]
    df.rename(columns={'FIPS': 'FIPS_CODE'}, inplace=True)
    ui_year = nearest_gridpoint(year, df['UI_YEAR'].unique())
    df = df[df['UI_YEAR'] == ui_year]
    return df[['FIPS_CODE', 'UI_CODE']]

# Cell

@memory.cache
def prepare_ruc_df(year):
    df = ers_codes.get_ruc_df()[['FIPS', 'RUC_YEAR', 'RUC_CODE']]
    df.rename(columns={'FIPS': 'FIPS_CODE'}, inplace=True)
    ruc_year = nearest_gridpoint(year, df['RUC_YEAR'].unique())
    df = df[df['RUC_YEAR'] == ruc_year]
    return df[['FIPS_CODE', 'RUC_CODE']]


# Cell

@memory.cache
def prepare_ruca_df(year):
    df = ers_codes.get_ruca_df()[['FIPS', 'YEAR', 'RUCA_CODE']]
    df.rename(columns={'FIPS': 'CENSUS_TRACT_FULL'}, inplace=True)
    ruca_year = nearest_gridpoint(year, df['YEAR'].unique())
    df = df[df['YEAR'] == ruca_year]
    return df[['CENSUS_TRACT_FULL', 'RUCA_CODE']]


# Cell

def gen_rural_hrsa(df):
    """Return bool column of rurality by HRSA definition for all establishments in `df`."""
    df_hrsa = pd.DataFrame(get_hrsa_rural_in_cbsa(), columns=['CENSUS_TRACT_FULL'])
    df = df[['CENSUS_TRACT_FULL', 'CBSA_LEVEL']].copy()
    df = df.merge(df_hrsa, 'left', 'CENSUS_TRACT_FULL', indicator=True)
    rural_in_cbsa = (df['_merge'] == 'both')
    rural_out_cbsa = df['CBSA_LEVEL'].isna()
    return rural_in_cbsa | rural_out_cbsa

def get_hrsa_rural_in_cbsa():
    """Return list of tracts that are in CBSA, but rural by HRSA definition."""
    tracts = []
    # FORHP list of 2300+ rural census tracts
    # This is a pre-processed text version of a former PDF file.
    with open('/InfoGroup/data/rurality/tract_data.txt', 'r') as fin:
        for line in fin:
            if line[0] != chr(32):
                continue
            else:
                line = line.strip()
                try:
                    if line[0].isnumeric():
                        tracts.append(line)
                except IndexError:
                    pass
    return tracts

# Cell

@memory.cache
def get_far_df():
    """Return dataframe with ERS FAR levels by Zip code."""
    far_file = '/InfoGroup/data/rurality/reference/FARcodesZIPdata2010WithAKandHI.xlsx'
    dtypes = {'ZIP': str, 'far1': int, 'far2': int, 'far3': int, 'far4': int}
    df = pd.read_excel(far_file, 'FAR ZIP Code Data', usecols=dtypes.keys(), dtype=dtypes)
    assert (df['ZIP'].str.len() == 5).all()
    assert df.isna().sum().sum() == 0
    # We can add up binary farX indicators to obtain single FAR level,
    # because classification is nested, e.g. far2 implies far1, and not-far1 implies not-far2.
    x = df['far1'] + df['far2'] + df['far3'] + df['far4']
    df['FAR_LEVEL'] = pd.Categorical(x, [0, 1, 2, 3, 4], True)
    df = df[['ZIP', 'FAR_LEVEL']].drop_duplicates()
    assert not df['ZIP'].duplicated().any()
    return df

# Cell

def add_rural_cols(year):
    """Load InfoGroup data for one `year`, add rurality columns and
    save as parquet partition.
    """

    logging.info(f'Start add_rural_cols({year})')

    df_outside_ua = prepare_outside_ua_df()
    df_ui = prepare_ui_df(year)
    df_ruc = prepare_ruc_df(year)
    df_ruca = prepare_ruca_df(year)
    df_far = get_far_df()

    cols = ['COMPANY', 'CITY', 'STATE', 'ZIP', 'COUNTY_CODE', 'SIC', 'NAICS',
            'YEAR', 'EMPLOYEES', 'SALES', 'ABI', 'PARENT_NUMBER',
            'CENSUS_TRACT', 'LATITUDE', 'LONGITUDE', 'CBSA_CODE', 'CBSA_LEVEL', 'CSA_CODE', 'FIPS_CODE']

    df = infogroup.get_df([year], cols)
    df.drop(columns=['YEAR'], inplace=True)
    df['CENSUS_TRACT_FULL'] = df['FIPS_CODE'] + df['CENSUS_TRACT']

    df = df.merge(df_outside_ua, 'left', 'CENSUS_TRACT_FULL', indicator=True)
    counts = df['_merge'].value_counts()
    logging.debug(f'Merge result of "RURAL_OUTSIDE_UA" in {year}\n{counts}\n')
    df.drop(columns=['_merge'], inplace=True)

    df = df.merge(df_ui, 'left', 'FIPS_CODE', indicator=True)
    counts = df['_merge'].value_counts()
    logging.debug(f'Merge result of "UI_CODE" in {year}\n{counts}\n')
    df.drop(columns=['_merge'], inplace=True)

    df = df.merge(df_ruc, 'left', 'FIPS_CODE', indicator=True)
    counts = df['_merge'].value_counts()
    logging.debug(f'Merge result of "RUC_CODE" in {year}\n{counts}\n')
    df.drop(columns=['_merge'], inplace=True)

    df = df.merge(df_ruca, 'left', 'CENSUS_TRACT_FULL', indicator=True)
    counts = df['_merge'].value_counts()
    logging.debug(f'Merge result of "RUCA_CODE" in {year}\n{counts}\n')
    df.drop(columns=['_merge'], inplace=True)

    df['RURAL_HRSA'] = gen_rural_hrsa(df)
    counts = df['RURAL_HRSA'].value_counts()
    logging.debug(f'Rural by HRSA in {year}\n{counts}\n')

    df = df.merge(df_far, 'left', 'ZIP', indicator=True)
    counts = df['_merge'].value_counts()
    logging.debug(f'Merge result of "FAR_LEVEL" in {year}\n{counts}\n')
    df.drop(columns=['_merge'], inplace=True)

    partition_path = str(resources.get('infogroup/rural').path / f'YEAR={year}')
    fastparquet.write(partition_path, df, file_scheme='hive', write_index=False, partition_on=['STATE'])

    logging.info(f'Finish add_rural_cols({year})')
    return partition_path


def build_parquet_dataset(n_cpus=1):
    """Merge rural columns to all years, save and merge parquet partitions."""

    logging.info(f'Start build_parquet_dataset({n_cpus})')

    p = resources.get('infogroup/rural').path
    # Remove dataset files if they exist from before
    if p.exists():
        shutil.rmtree(p)
    p.mkdir()

    data_years = range(1997, 2018)
    with mp.Pool(n_cpus) as pool:
        partition_paths = pool.map(add_rural_cols, data_years)
    _ = fastparquet.writer.merge(partition_paths)

    logging.info(f'Finish build_parquet_dataset({n_cpus})')

# Cell

def get_df(years=None, cols=None, states=None, onlymeta=False):
    """Return InfoGroup with rurality columns as dataframe.
    If `onlymeta` is True, return ParquetFile instead, which can be used
    to quickly inspect dataset schema without loading it ("dtypes" attribute).
    """
    path = resources.get('infogroup/rural').path
    if onlymeta:
        return fastparquet.ParquetFile(str(path))

    filters = []
    if years is not None:
        filters.append(('YEAR', 'in', years))
    if states is not None:
        filters.append(('STATE', 'in', states))

    df = pd.read_parquet(path, 'fastparquet', columns=cols, filters=filters)

    if cols is None or 'YEAR' in cols:
        df['YEAR'] = df['YEAR'].astype(int)
    else:
        df.drop(columns=['YEAR'], inplace=True)

    if not (cols is None or 'STATE' in cols):
        df.drop(columns=['STATE'], inplace=True)

    if cols is None or 'FAR_LEVEL' in cols:
        df['FAR_LEVEL'].cat.as_ordered(inplace=True)

    return df