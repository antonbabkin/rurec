{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGroup data\n",
    "\n",
    "> Process and prepare InfoGroup dataset.\n",
    "\n",
    "## Processing\n",
    "\n",
    "Starting from original CSV files.\n",
    "\n",
    "- Convert to unicode\n",
    "- Validate against JSON schema. A few erroneous data entries are erased here (e.g. text in numerical column). Existing implementation uses datapackage validator and takes several days with single core.\n",
    "- Save to disk in parquet format.\n",
    "- Provide interface to load single year of data. Allow filtering, column selection and small (optionally random) sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp infogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "from IPython import display\n",
    "\n",
    "from rurec import resources\n",
    "from rurec.resources import Resource\n",
    "from rurec import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=resources.get('infogroup/schema').path.parent/'processing.log', \n",
    "                    filemode='w', level=logging.INFO, format='%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_years = range(1997, 2018)\n",
    "resources.add(Resource('infogroup/schema', '/InfoGroup/data/processed/schema.json', 'Processed InfoGroup data, schema', False))\n",
    "for y in data_years:\n",
    "    resources.add(Resource(f'infogroup/csv/{y}', f'/InfoGroup/data/processed/{y}.csv', f'Processed InfoGroup data, {y}, CSV format', False))\n",
    "    resources.add(Resource(f'infogroup/pq/{y}', f'/InfoGroup/data/processed/{y}.pq', f'Processed InfoGroup data, {y}, parquet format', False))\n",
    "    resources.add(Resource(f'infogroup/orig/{y}', f'/InfoGroup/data/original/raw/{y}_Business_Academic_QCQ.csv', f'Original unprocessed InfoGroup data, {y}', False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear and validate raw data\n",
    "\n",
    "- Change \"latin-1\" encoding to \"utf-8\", remove double quotes around values.\n",
    "- Remove double quotes around every value.\n",
    "- Rename columns to ALL_CAPS.\n",
    "- In 2009: pad string fields with zeroes.\n",
    "- Validate values format, replace errors with missing values.\n",
    "\n",
    "### Future work\n",
    "\n",
    "- Correct 2-digit state part of the FIPS code.\n",
    "- Correct missing CBSA code and CBSA level, mainly in 2009.\n",
    "- add indicator variables for different samples (random, WI, FAI, ...) to be used as parquet partitions to allow quick read of data subsets\n",
    "- put meaningfult labels to categoricals (\"1-5\" instead if \"A\" etc).\n",
    "  - This will be tricky for POPULATION_CODE that changes coding between 2015 and 2016\n",
    "- make up and add enum constraints for TITLE_CODE and CALL_STATUS_CODE\n",
    "- add logging of errors to a file\n",
    "- if categoricals are worthy on fields with large number of unique values, possible unknown a priori, such as city or NAICS, then they should be applied. care should be taken because set of unique values can vary between years, and it might create problems when merging.\n",
    "- validations:\n",
    "  - codes are valid (i.e. can be found in lookup tables) for fields such as SIC, NAICS, FIPS, CBSA_CODE etc.\n",
    "  - geo variable consistency: CBSA_LEVEL vs CBSA_CODE, lon-lat, nesting of areas\n",
    "- few variables have many values like \"00000\", those should possibly be replaced with np.nan\n",
    "  - subsidiary_number, parent_number, site_number, census_tract, csa_code, maybe others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic information\n",
    "\n",
    "- ADDRESS: historical address\n",
    "- CITY: historical address city\n",
    "- STATE: historical address state\n",
    "- ZIP: historical address zip code\n",
    "- ZIP4: historical address zip code zip + 4\n",
    "- COUNTY_CODE: county code based upon location address/zip4 (postal)\n",
    "- AREA_CODE: area code of business\n",
    "- ADDRESS_TYPE: indicates if type of address. \"F\": \"Firm\", \"G\": \"General delivery\", \"H\": \"High-rise\", \"M\": \"Military\", \"P\": \"Post office box\", \"R\": \"Rural route or hwy contract\", \"S\": \"Street\", \"N\": \"Unknown\", \"\": \"No match to Zip4\".\n",
    "- CENSUS_TRACT: identifies a small geographic area for the purpose of collecting and compiling population and housing data.  census tracts are unique only within census county, and census counties are unique only within census state.  \n",
    "- CENSUS_BLOCK: bgs are subdivisions of census tracts and unique only within a specific census tract.  census tracts/block groups are assigned to address records via a geocoding process.\n",
    "- LATITUDE: parcel level assigned via point geo coding.  half of a pair of coordinates (the other being longitude)  provided in a formatted value, with decimals or a negative sign. not available in puerto rico & virgin island.\n",
    "- LONGITUDE: parcel level assigned via point geo coding.  note: longitudes are negatives values in the western hemisphere.  provided in its formatted value, with decimals or a negative sign. not available in puerto rico & virigin island\n",
    "- MATCH_CODE: parcel level match code of the business location. \"0\": \"Site level\", \"2\": \"Zip+2 centroid\", \"4\": \"Zip+4 centroid\", \"P\": \"Parcel\", \"X\": \"Zip centroid\".\n",
    "- CBSA_CODE: core bases statistical area (expanded msa code)\n",
    "- CBSA_LEVEL: indicates if an area is a micropolitan or metropolitan area. \"1\": \"Micropolitan\", \"2\": \"Metropolitan\"\n",
    "- CSA_CODE: adjoining cbsa's.  combination of metro and micro areas\n",
    "- FIPS_CODE: first 2 bytes = state code, last 3 bytes = county code (location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def convert_schema(datapackage_schema_path):\n",
    "    \"\"\"Convert old datapackage schema.json into a new file, to be used for data validation.\"\"\"\n",
    "    sch0 = json.load(open(datapackage_schema_path))\n",
    "\n",
    "    def get_field_years(field_name):\n",
    "        years = []\n",
    "        for fl in sch0['field_lists']:\n",
    "            if field_name in fl['fields']:\n",
    "                years += fl['years']\n",
    "        if 2015 in years:\n",
    "            years += [2016, 2017]\n",
    "        if field_name == 'gender':\n",
    "            years += [2017]\n",
    "        return sorted(years)\n",
    "\n",
    "\n",
    "    sch = dict()\n",
    "    sch['info'] = 'Schema for cleaned InfoGroup data.'\n",
    "    sch['fields'] = list()\n",
    "\n",
    "    for f0 in sch0['fields']:\n",
    "        f = dict()\n",
    "        name = f0['name']\n",
    "        f['name'] = name.upper()\n",
    "        f['years'] = get_field_years(name)\n",
    "        if 'enum' in f0['constraints']:\n",
    "            enum = f0['constraints']['enum'].copy()\n",
    "            if '' in enum:\n",
    "                enum.pop(enum.index(''))\n",
    "            f['enum'] = enum\n",
    "        if 'values' in f0:\n",
    "            values = f0['values'].copy()\n",
    "            if name == 'cbsa_level':\n",
    "                del values['0'] # code never used\n",
    "            f['enum_labels'] = values\n",
    "\n",
    "        field_widths = {\n",
    "            'ZIP': 5,\n",
    "            'ZIP4': 4,\n",
    "            'COUNTY_CODE': 3,\n",
    "            'AREA_CODE': 3,\n",
    "            'PHONE': 7,\n",
    "            'SIC': 6,\n",
    "            'SIC0': 6,\n",
    "            'SIC1': 6,\n",
    "            'SIC2': 6,\n",
    "            'SIC3': 6,\n",
    "            'SIC4': 6,\n",
    "            'NAICS': 8,\n",
    "            'YP_CODE': 5,\n",
    "            'ABI': 9,\n",
    "            'SUBSIDIARY_NUMBER': 9,\n",
    "            'PARENT_NUMBER': 9,\n",
    "            'SITE_NUMBER': 9,\n",
    "            'CENSUS_TRACT': 6,\n",
    "            'CENSUS_BLOCK': 1,\n",
    "            'CBSA_CODE': 5,\n",
    "            'CSA_CODE': 3,\n",
    "            'FIPS_CODE': 5\n",
    "        }\n",
    "        if f['name'] in field_widths:\n",
    "            f['width'] = field_widths[f['name']]\n",
    "\n",
    "        f['original_name'] = f0['originalName']\n",
    "        f['original_description'] = f0['originalDescription']\n",
    "        sch['fields'].append(f)\n",
    "\n",
    "    json.dump(sch, open(resources.get('infogroup/schema').path, 'w'), indent=1)\n",
    "    \n",
    "\n",
    "def pad_with_zeroes(df, schema_fields):\n",
    "    \"\"\"Prepend string column values with zeroes to have constant width.\"\"\"\n",
    "\n",
    "    for field in schema_fields:\n",
    "        if 'width' in field:\n",
    "            df[field['name']] = df[field['name']].str.zfill(field['width'])\n",
    "\n",
    "def validate_raw_strings(df, schema_fields):\n",
    "    \"\"\"Validate values in raw InfoGroup data according to string constraints.\n",
    "    Return list of dicts of invalid values.\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = {\n",
    "        'ZIP': {'number': True},\n",
    "        'ZIP4': {'number': True},\n",
    "        'COUNTY_CODE': {'number': True},\n",
    "        'AREA_CODE': {'number': True},\n",
    "        'PHONE': {'number': True},\n",
    "        'SIC': {'number': True},\n",
    "        'SIC0': {'number': True},\n",
    "        'SIC1': {'number': True},\n",
    "        'SIC2': {'number': True},\n",
    "        'SIC3': {'number': True},\n",
    "        'SIC4': {'number': True},\n",
    "        'NAICS': {'number': True},\n",
    "        'YEAR': {'notna': True, 'number': True},\n",
    "        'YP_CODE': {'number': True},\n",
    "        'EMPLOYEES': {'number': True},\n",
    "        'SALES': {'number': True},\n",
    "        'PARENT_EMPLOYEES': {'number': True},\n",
    "        'PARENT_SALES': {'number': True},\n",
    "        'YEAR_EST': {'number': True},\n",
    "        'ABI': {'unique': True, 'notna': True, 'number': True},\n",
    "        'SUBSIDIARY_NUMBER': {'number': True},\n",
    "        'PARENT_NUMBER': {'number': True},\n",
    "        'SITE_NUMBER': {'number': True},\n",
    "        'CENSUS_TRACT': {'number': True},\n",
    "        'CENSUS_BLOCK': {'number': True},\n",
    "        'LATITUDE': {'number': True},\n",
    "        'LONGITUDE': {'number': True},\n",
    "        'CBSA_CODE': {'number': True},\n",
    "        'CSA_CODE': {'number': True},\n",
    "        'FIPS_CODE': {'number': True}\n",
    "    }\n",
    "    \n",
    "    # the above hard coded list of constraints must be consistent with field availability in given year\n",
    "    constraints = {k: v for k, v in constraints.items() if k in df}\n",
    "    \n",
    "    \n",
    "    for field in schema_fields:\n",
    "        name = field['name']\n",
    "        if 'enum' in field:\n",
    "            if name not in constraints: constraints[name] = dict()\n",
    "            constraints[name]['cats'] = field['enum']\n",
    "        if 'width' in field:\n",
    "            constraints[name]['nchar'] = field['width']\n",
    "    \n",
    "    return util.validate_values(df, constraints)\n",
    "\n",
    "\n",
    "def convert_dtypes(df, schema_fields):\n",
    "    \"\"\"Inplace convert string columns to appropriate types.\"\"\"\n",
    "    \n",
    "    for col in ['YEAR', 'EMPLOYEES', 'SALES', 'PARENT_EMPLOYEES', 'PARENT_SALES', 'YEAR_EST', 'LATITUDE', 'LONGITUDE']:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "        \n",
    "    for field in schema_fields:\n",
    "        if 'enum' in field:\n",
    "            df[field['name']] = pd.Categorical(df[field['name']], categories=field['enum'])\n",
    "    \n",
    "\n",
    "def validate_raw_numbers(df):\n",
    "    \"\"\"Validate values in raw InfoGroup data according to numerical constraints.\n",
    "    Return list of dicts of invalid values.\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = {\n",
    "        'YEAR': {'eq': year},\n",
    "        'EMPLOYEES': {'ge': 0},\n",
    "        'SALES': {'ge': 0},\n",
    "        'PARENT_EMPLOYEES': {'ge': 0},\n",
    "        'PARENT_SALES': {'ge': 0},\n",
    "        'YEAR_EST': {'ge': 1000, 'le': year},\n",
    "        'LATITUDE': {'ge': 0, 'le': 90},\n",
    "        'LONGITUDE': {'ge': -180, 'le': 0}\n",
    "    }\n",
    "    return util.validate_values(df, constraints)\n",
    "\n",
    "def replace_invalid(df, invalid_list, replacement=np.nan):\n",
    "    \"\"\"Replace invalid values.\"\"\"\n",
    "    for inv in invalid_list:\n",
    "        df.loc[inv['idx'], inv['col']] = replacement\n",
    "        logging.info(f'Replace invalid value `{inv[\"val\"]}` with `{replacement}` at .loc[{inv[\"idx\"]}, \\'{inv[\"col\"]}\\']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in data_years:\n",
    "for year in range(1997, 2018):\n",
    "    logging.info(f'\\nProcessing started for year {year}\\n' + '-'*80)\n",
    "    sch = json.load(resources.get(f'infogroup/schema').path.open())\n",
    "    sch = [x for x in sch['fields'] if year in x['years']]\n",
    "    \n",
    "    # POPULATION_CODE has different values in 2016 and 2017\n",
    "    if year >= 2016:\n",
    "        for f in sch:\n",
    "            if f['name'] == 'POPULATION_CODE':\n",
    "                f['enum'] = list('0123456789')\n",
    "                break\n",
    "\n",
    "    df = pd.read_csv(resources.get(f'infogroup/orig/{year}').path, dtype='str', encoding='latin-1')\n",
    "\n",
    "    df.rename(columns={x['original_name']: x['name'] for x in sch}, inplace=True)\n",
    "\n",
    "    if year == 2009:\n",
    "        pad_with_zeroes(df, sch)\n",
    "\n",
    "    invalid_str = validate_raw_strings(df, sch)\n",
    "    if len(invalid_str) < 100:\n",
    "        replace_invalid(df, invalid_str)\n",
    "    else:\n",
    "        logging.error(f'Very many invalid_str values: {len(invalid_str)}, processing aborted')\n",
    "        logging.error(invalid_str[:5])\n",
    "        continue # skip to next year\n",
    "\n",
    "    convert_dtypes(df, sch)\n",
    "\n",
    "    invalid_num = validate_raw_numbers(df)\n",
    "    if len(invalid_num) < 100:\n",
    "        replace_invalid(df, invalid_num)\n",
    "    else:\n",
    "        logging.error(f'Very many invalid_num values: {len(invalid_num)}, processing aborted')\n",
    "        logging.error(invalid_num[:5])\n",
    "        continue # skip to next year\n",
    "\n",
    "    df.to_csv(resources.get(f'infogroup/csv/{year}').path, index=False)\n",
    "    fastparquet.write(resources.get(f'infogroup/pq/{year}').path, df, write_index=False)\n",
    "    logging.info(f'\\nProcessing finished for year {year}\\n' + '-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_df(year, cols=None):\n",
    "    \"\"\"Return one year of InfoGroup data with appropriate data types.\n",
    "    Subset of columns can be loaded by passing list to `cols`.\n",
    "    \"\"\"\n",
    "    res = resources.get(f'infogroup/pq/{year}')\n",
    "    return pd.read_parquet(res.path, 'fastparquet', columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
