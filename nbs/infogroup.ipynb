{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGroup data\n",
    "\n",
    "> Specify schema and convert raw CSV to a partitioned parquet dataset.\n",
    "\n",
    "- Start with original data files in CSV format.\n",
    "- Upgrade old datapackage.json schema.\n",
    "- Rename columns and standardize to ALL_CAPS.\n",
    "- In 2009: pad string fields with zeroes.\n",
    "- Validate values format, replace errors with missing values.\n",
    "- Save to disk in parquet format.\n",
    "  - Excluding variables that are not present in all years and POPULATION_CODE.\n",
    "- Provide interface to load data as DataFrame, with options to select columns and subsamples by year and state.\n",
    "\n",
    "## Possible future work\n",
    "\n",
    "- add indicator variables for different samples (random, WI, FAI, ...) to be used as parquet partitions to allow quick read of data subsets\n",
    "- put meaningfult labels to categoricals (\"1-5\" instead if \"A\" etc).\n",
    "  - This will be tricky for POPULATION_CODE that changes coding between 2015 and 2016\n",
    "- make up and add enum constraints for TITLE_CODE and CALL_STATUS_CODE\n",
    "- if categoricals are worthy on fields with large number of unique values, possible unknown a priori, such as city or NAICS, then they should be applied. care should be taken because set of unique values can vary between years, and it might create problems when merging.\n",
    "- few variables have many values like \"00000\", those should possibly be replaced with np.nan\n",
    "  - subsidiary_number, parent_number, site_number, census_tract, csa_code, maybe others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "%nbdev_default_export infogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "from IPython import display\n",
    "from joblib import Memory\n",
    "\n",
    "from rurec import resources\n",
    "from rurec.resources import Resource\n",
    "from rurec import util\n",
    "\n",
    "memory = Memory(resources.paths.cache)\n",
    "data_years = range(1997, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# for batch runs: log to a file\n",
    "logging.basicConfig(filename=resources.paths.root / 'logs/processing.log', \n",
    "                    filemode='w', level=logging.INFO, format='%(asctime)s %(levelname)s:\\n%(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interactive use: log to stdout\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s %(levelname)s:\\n%(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources.add(Resource('infogroup/schema', '/InfoGroup/data/processed/schema.json', 'Processed InfoGroup data, schema', False))\n",
    "resources.add(Resource('infogroup/full', '/InfoGroup/data/processed/full.pq', 'Processed InfoGroup data, all years, partitioned parquet', False))\n",
    "for y in data_years:\n",
    "    resources.add(Resource(f'infogroup/orig/{y}', f'/InfoGroup/data/original/raw/{y}_Business_Academic_QCQ.csv', f'Original unprocessed InfoGroup data, {y}', False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override resource paths for testing and debugging\n",
    "resources.get('infogroup/full').path = resources.paths.root / 'tmp/full.pq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster returns for testing and debugging\n",
    "pd.read_csv = functools.partial(pd.read_csv, nrows=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "def convert_schema(datapackage_schema_path):\n",
    "    \"\"\"Convert old datapackage schema.json into a new file, to be used for data validation.\"\"\"\n",
    "    sch0 = json.load(open(datapackage_schema_path))\n",
    "\n",
    "    def get_field_years(field_name):\n",
    "        years = []\n",
    "        for fl in sch0['field_lists']:\n",
    "            if field_name in fl['fields']:\n",
    "                years += fl['years']\n",
    "        if 2015 in years:\n",
    "            years += [2016, 2017]\n",
    "        if field_name == 'gender':\n",
    "            years += [2017]\n",
    "        return sorted(years)\n",
    "\n",
    "\n",
    "    sch = dict()\n",
    "    sch['info'] = 'Schema for cleaned InfoGroup data.'\n",
    "    sch['fields'] = list()\n",
    "\n",
    "    for f0 in sch0['fields']:\n",
    "        f = dict()\n",
    "        name = f0['name']\n",
    "        f['name'] = name.upper()\n",
    "        f['years'] = get_field_years(name)\n",
    "        if 'enum' in f0['constraints']:\n",
    "            enum = f0['constraints']['enum'].copy()\n",
    "            if '' in enum:\n",
    "                enum.pop(enum.index(''))\n",
    "            f['enum'] = enum\n",
    "        if 'values' in f0:\n",
    "            values = f0['values'].copy()\n",
    "            if name == 'cbsa_level':\n",
    "                del values['0'] # code never used\n",
    "            f['enum_labels'] = values\n",
    "\n",
    "        field_widths = {\n",
    "            'ZIP': 5,\n",
    "            'ZIP4': 4,\n",
    "            'COUNTY_CODE': 3,\n",
    "            'AREA_CODE': 3,\n",
    "            'PHONE': 7,\n",
    "            'SIC': 6,\n",
    "            'SIC0': 6,\n",
    "            'SIC1': 6,\n",
    "            'SIC2': 6,\n",
    "            'SIC3': 6,\n",
    "            'SIC4': 6,\n",
    "            'NAICS': 8,\n",
    "            'YP_CODE': 5,\n",
    "            'ABI': 9,\n",
    "            'SUBSIDIARY_NUMBER': 9,\n",
    "            'PARENT_NUMBER': 9,\n",
    "            'SITE_NUMBER': 9,\n",
    "            'CENSUS_TRACT': 6,\n",
    "            'CENSUS_BLOCK': 1,\n",
    "            'CBSA_CODE': 5,\n",
    "            'CSA_CODE': 3,\n",
    "            'FIPS_CODE': 5\n",
    "        }\n",
    "        if f['name'] in field_widths:\n",
    "            f['width'] = field_widths[f['name']]\n",
    "\n",
    "        f['original_name'] = f0['originalName']\n",
    "        f['original_description'] = f0['originalDescription']\n",
    "        sch['fields'].append(f)\n",
    "\n",
    "    json.dump(sch, open(resources.get('infogroup/schema').path, 'w'), indent=1)\n",
    "    \n",
    "\n",
    "def get_schema(field_name=None, year=None):\n",
    "    \"\"\"Flexible function to get dataset schema.\n",
    "    \n",
    "    With no parameters, return full list of field dictionaries.\n",
    "    With `year`, restrict to fields present in that year.\n",
    "    With `field_name`, return dictionary for specific field (`year` is ignored in this case).\n",
    "    \"\"\"\n",
    "    sch = json.load(resources.get(f'infogroup/schema').path.open())['fields']\n",
    "    if field_name is not None:\n",
    "        return [x for x in sch if x['name'] == field_name][0]\n",
    "    if year is not None:\n",
    "        return [x for x in sch if year in x['years']]\n",
    "    return sch  \n",
    "\n",
    "    \n",
    "def pad_with_zeroes(df, schema_fields):\n",
    "    \"\"\"Prepend string column values with zeroes to have constant width.\"\"\"\n",
    "\n",
    "    for field in schema_fields:\n",
    "        if 'width' in field:\n",
    "            df[field['name']] = df[field['name']].str.zfill(field['width'])\n",
    "\n",
    "def validate_raw_strings(df, schema_fields):\n",
    "    \"\"\"Validate values in raw InfoGroup data according to string constraints.\n",
    "    Return list of dicts of invalid values.\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = {\n",
    "        'ZIP': {'number': True},\n",
    "        'ZIP4': {'number': True},\n",
    "        'COUNTY_CODE': {'number': True},\n",
    "        'AREA_CODE': {'number': True},\n",
    "        'PHONE': {'number': True},\n",
    "        'SIC': {'number': True},\n",
    "        'SIC0': {'number': True},\n",
    "        'SIC1': {'number': True},\n",
    "        'SIC2': {'number': True},\n",
    "        'SIC3': {'number': True},\n",
    "        'SIC4': {'number': True},\n",
    "        'NAICS': {'number': True},\n",
    "        'YEAR': {'notna': True, 'number': True},\n",
    "        'YP_CODE': {'number': True},\n",
    "        'EMPLOYEES': {'number': True},\n",
    "        'SALES': {'number': True},\n",
    "        'PARENT_EMPLOYEES': {'number': True},\n",
    "        'PARENT_SALES': {'number': True},\n",
    "        'YEAR_EST': {'number': True},\n",
    "        'ABI': {'unique': True, 'notna': True, 'number': True},\n",
    "        'SUBSIDIARY_NUMBER': {'number': True},\n",
    "        'PARENT_NUMBER': {'number': True},\n",
    "        'SITE_NUMBER': {'number': True},\n",
    "        'CENSUS_TRACT': {'number': True},\n",
    "        'CENSUS_BLOCK': {'number': True},\n",
    "        'LATITUDE': {'number': True},\n",
    "        'LONGITUDE': {'number': True},\n",
    "        'CBSA_CODE': {'number': True},\n",
    "        'CSA_CODE': {'number': True},\n",
    "        'FIPS_CODE': {'number': True}\n",
    "    }\n",
    "    \n",
    "    # the above hard coded list of constraints must be consistent with field availability in given year\n",
    "    constraints = {k: v for k, v in constraints.items() if k in df}\n",
    "    \n",
    "    \n",
    "    for field in schema_fields:\n",
    "        name = field['name']\n",
    "        if 'enum' in field:\n",
    "            if name not in constraints: constraints[name] = dict()\n",
    "            constraints[name]['cats'] = field['enum']\n",
    "        if 'width' in field:\n",
    "            constraints[name]['nchar'] = field['width']\n",
    "    \n",
    "    return util.validate_values(df, constraints)\n",
    "\n",
    "\n",
    "def convert_dtypes(df, schema_fields):\n",
    "    \"\"\"Inplace convert string columns to appropriate types.\"\"\"\n",
    "    \n",
    "    for col in ['YEAR', 'EMPLOYEES', 'SALES', 'PARENT_EMPLOYEES', 'PARENT_SALES', 'YEAR_EST', 'LATITUDE', 'LONGITUDE']:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "        \n",
    "    for field in schema_fields:\n",
    "        if 'enum' in field:\n",
    "            df[field['name']] = pd.Categorical(df[field['name']], categories=field['enum'])\n",
    "    \n",
    "\n",
    "def validate_raw_numbers(df, year):\n",
    "    \"\"\"Validate values in raw InfoGroup data according to numerical constraints.\n",
    "    Return list of dicts of invalid values.\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = {\n",
    "        'YEAR': {'eq': year},\n",
    "        'EMPLOYEES': {'ge': 0},\n",
    "        'SALES': {'ge': 0},\n",
    "        'PARENT_EMPLOYEES': {'ge': 0},\n",
    "        'PARENT_SALES': {'ge': 0},\n",
    "        'YEAR_EST': {'ge': 1000, 'le': year},\n",
    "        'LATITUDE': {'ge': 0, 'le': 90},\n",
    "        'LONGITUDE': {'ge': -180, 'le': 0}\n",
    "    }\n",
    "    return util.validate_values(df, constraints)\n",
    "\n",
    "def replace_invalid(df, invalid_list, replacement=np.nan):\n",
    "    \"\"\"Replace invalid values.\"\"\"\n",
    "    for inv in invalid_list:\n",
    "        df.loc[inv['idx'], inv['col']] = replacement\n",
    "        logging.info(f'Replace invalid value `{inv[\"val\"]}` with `{replacement}` at .loc[{inv[\"idx\"]}, \\'{inv[\"col\"]}\\']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_schema('/InfoGroup/data/original/raw/datapackage_schema.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Columns that do not appear in all years, as well as POPULATION_CODE that changes categories,\n",
    "# are not includes in the full parquet dataset\n",
    "shared_cols = [x['name']\n",
    "               for x in json.load(resources.get('infogroup/schema').path.open())['fields']\n",
    "               if (1997 in x['years']) and (x['name'] not in ['YEAR', 'POPULATION_CODE'])]\n",
    "\n",
    "def load_validate_convert(year):\n",
    "    \"\"\"Run full processing pipeline on one year of data:\n",
    "    load CVS, validate values, convert dtypes and save as parquet partition.\n",
    "    \"\"\"\n",
    "    logging.info(f'Processing started for year {year}\\n' + '-'*80)\n",
    "    sch = json.load(resources.get(f'infogroup/schema').path.open())\n",
    "    sch = [x for x in sch['fields'] if year in x['years']]\n",
    "    \n",
    "    # POPULATION_CODE has different values in 2016 and 2017\n",
    "    if year >= 2016:\n",
    "        for f in sch:\n",
    "            if f['name'] == 'POPULATION_CODE':\n",
    "                f['enum'] = list('0123456789')\n",
    "                break\n",
    "\n",
    "    df = pd.read_csv(resources.get(f'infogroup/orig/{year}').path, dtype='str', encoding='latin-1')\n",
    "\n",
    "    df.rename(columns={x['original_name']: x['name'] for x in sch}, inplace=True)\n",
    "\n",
    "    if year == 2009:\n",
    "        pad_with_zeroes(df, sch)\n",
    "\n",
    "    invalid_str = validate_raw_strings(df, sch)\n",
    "    if len(invalid_str) < 100:\n",
    "        replace_invalid(df, invalid_str)\n",
    "    else:\n",
    "        logging.error(f'Very many invalid_str values: {len(invalid_str)}, processing aborted')\n",
    "        logging.error(invalid_str[:5])\n",
    "        return\n",
    "\n",
    "    convert_dtypes(df, sch)\n",
    "\n",
    "    invalid_num = validate_raw_numbers(df, year)\n",
    "    if len(invalid_num) < 100:\n",
    "        replace_invalid(df, invalid_num)\n",
    "    else:\n",
    "        logging.error(f'Very many invalid_num values: {len(invalid_num)}, processing aborted')\n",
    "        logging.error(invalid_num[:5])\n",
    "        return\n",
    "    \n",
    "    df = df[shared_cols]\n",
    "    partition_path = str(resources.get('infogroup/full').path / f'YEAR={year}')\n",
    "    fastparquet.write(partition_path, df, file_scheme='hive', write_index=False, partition_on=['STATE'])\n",
    "\n",
    "    logging.info(f'Processing finished for year {year}\\n' + '-'*80)\n",
    "    return partition_path\n",
    "\n",
    "\n",
    "def build_parquet_dataset(n_cpus=1):\n",
    "    \"\"\"Create full parquet dataset from yearly CSV files.\"\"\"\n",
    "    \n",
    "    logging.info('create_parquet_dataset() started.')\n",
    "    \n",
    "    p = resources.get('infogroup/full').path\n",
    "    # Remove dataset files if they exist from before\n",
    "    if p.exists():\n",
    "        shutil.rmtree(p)\n",
    "    p.mkdir()\n",
    "    if n_cpus > 1:\n",
    "        with mp.Pool(n_cpus) as pool:\n",
    "            partition_paths = pool.map(load_validate_convert, data_years)\n",
    "    else:\n",
    "        partition_paths = [load_validate_convert(y) for y in data_years]\n",
    "    _ = fastparquet.writer.merge(partition_paths)\n",
    "    logging.info('create_parquet_dataset() finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def get_df(years=None, cols=None, states=None):\n",
    "    \"\"\"Return one year of InfoGroup data with appropriate data types.\n",
    "    Subset of columns can be loaded by passing list to `cols`.\n",
    "    \"\"\"\n",
    "    filters = []\n",
    "    if years is not None:\n",
    "        filters.append(('YEAR', 'in', years))\n",
    "    if states is not None:\n",
    "        filters.append(('STATE', 'in', states))\n",
    "    res = resources.get('infogroup/full')\n",
    "    df = pd.read_parquet(res.path, 'fastparquet', columns=cols, filters=filters)\n",
    "    df['YEAR'] = df['YEAR'].astype(int)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
