{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGroup data\n",
    "\n",
    "> Process and prepare InfoGroup dataset.\n",
    "\n",
    "## Processing\n",
    "\n",
    "Starting from original CSV files.\n",
    "\n",
    "- Convert to unicode\n",
    "- Validate against JSON schema. A few erroneous data entries are erased here (e.g. text in numerical column). Existing implementation uses datapackage validator and takes several days with single core.\n",
    "- Save to disk in parquet format.\n",
    "- Provide interface to load single year of data. Allow filtering, column selection and small (optionally random) sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp infogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "from IPython import display\n",
    "\n",
    "from rurec import resources\n",
    "from rurec.resources import Resource\n",
    "from rurec import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources.add(Resource('infogroup/schema', '/InfoGroup/data/processed/schema.json', 'Processed InfoGroup data, schema', False))\n",
    "for y in range(1997, 2018):\n",
    "    resources.add(Resource(f'infogroup/csv/{y}', f'/InfoGroup/data/processed/{y}.csv', f'Processed InfoGroup data, {y}, CSV format', False))\n",
    "    resources.add(Resource(f'infogroup/pq/{y}', f'/InfoGroup/data/processed/{y}.pq', f'Processed InfoGroup data, {y}, parquet format', False))\n",
    "    resources.add(Resource(f'infogroup/orig/{y}', f'/InfoGroup/data/original/raw/{y}_Business_Academic_QCQ.csv', f'Original unprocessed InfoGroup data, {y}', False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear and validate raw data\n",
    "\n",
    "- Change \"latin-1\" encoding to \"utf-8\", remove double quotes around values.\n",
    "- Remove double quotes around every value.\n",
    "- Rename columns to ALL_CAPS.\n",
    "- In 2009: pad string fields with zeroes.\n",
    "- Validate values format, replace errors with missing values.\n",
    "\n",
    "### Future work\n",
    "\n",
    "- Correct 2-digit state part of the FIPS code.\n",
    "- Correct missing CBSA code and CBSA level, mainly in 2009.\n",
    "- add indicator variables for different samples (random, WI, FAI, ...) to be used as parquet partitions to allow quick read of data subsets\n",
    "- put meaningfult labels to categoricals (\"1-5\" instead if \"A\" etc).\n",
    "  - This will be tricky for POPULATION_CODE that changes coding between 2015 and 2016\n",
    "- read through schema file and see if there is anything useful left in notes or elswhere\n",
    "- make up and add enum constraints for TITLE_CODE and CALL_STATUS_CODE\n",
    "- add logging of errors to a file\n",
    "- if categoricals are worthy on fields with large number of unique values, possible unknown a priori, such as city or NAICS, then they should be applied. care should be taken because set of unique values can vary between years, and it might create problems when merging.\n",
    "- further validations:\n",
    "  - codes are valid (i.e. can be found in lookup tables) for fields such as SIC, NAICS, FIPS, CBSA_CODE etc.\n",
    "  - geo variable consistency: CBSA_LEVEL vs CBSA_CODE, lon-lat, nesting of areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def convert_schema(datapackage_schema_path):\n",
    "    \"\"\"Convert old datapackage schema.json into a new file, to be used for data validation.\"\"\"\n",
    "    sch0 = json.load(open(datapackage_schema_path))\n",
    "\n",
    "    def get_field_years(field_name):\n",
    "        years = []\n",
    "        for fl in sch0['field_lists']:\n",
    "            if field_name in fl['fields']:\n",
    "                years += fl['years']\n",
    "        if 2015 in years:\n",
    "            years += [2016, 2017]\n",
    "        return sorted(years)\n",
    "\n",
    "\n",
    "    sch = dict()\n",
    "    sch['info'] = 'Schema for cleaned InfoGroup data.'\n",
    "    sch['fields'] = list()\n",
    "\n",
    "    for f0 in sch0['fields']:\n",
    "        f = dict()\n",
    "        name = f0['name']\n",
    "        f['name'] = name.upper()\n",
    "        f['years'] = get_field_years(name)\n",
    "        if 'enum' in f0['constraints']:\n",
    "            enum = f0['constraints']['enum'].copy()\n",
    "            if '' in enum:\n",
    "                enum.pop(enum.index(''))\n",
    "            f['enum'] = enum\n",
    "        if 'values' in f0:\n",
    "            values = f0['values'].copy()\n",
    "            if name == 'cbsa_level':\n",
    "                del values['0'] # code never used\n",
    "            f['enum_labels'] = values\n",
    "\n",
    "        field_widths = {\n",
    "            'ZIP': 5,\n",
    "            'ZIP4': 4,\n",
    "            'COUNTY_CODE': 3,\n",
    "            'AREA_CODE': 3,\n",
    "            'PHONE': 7,\n",
    "            'SIC': 6,\n",
    "            'SIC0': 6,\n",
    "            'SIC1': 6,\n",
    "            'SIC2': 6,\n",
    "            'SIC3': 6,\n",
    "            'SIC4': 6,\n",
    "            'NAICS': 8,\n",
    "            'YP_CODE': 5,\n",
    "            'ABI': 9,\n",
    "            'SUBSIDIARY_NUMBER': 9,\n",
    "            'PARENT_NUMBER': 9,\n",
    "            'SITE_NUMBER': 9,\n",
    "            'CENSUS_TRACT': 6,\n",
    "            'CENSUS_BLOCK': 1,\n",
    "            'CBSA_CODE': 5,\n",
    "            'CSA_CODE': 3,\n",
    "            'FIPS_CODE': 5\n",
    "        }\n",
    "        if f['name'] in field_widths:\n",
    "            f['width'] = field_widths[f['name']]\n",
    "\n",
    "        f['original_name'] = f0['originalName']\n",
    "        f['original_description'] = f0['originalDescription']\n",
    "        sch['fields'].append(f)\n",
    "\n",
    "    json.dump(sch, open(resources.get('infogroup/schema').path, 'w'), indent=1)\n",
    "    \n",
    "\n",
    "def pad_with_zeroes(df, schema_fields):\n",
    "    \"\"\"Prepend string column values with zeroes to have constant width.\"\"\"\n",
    "\n",
    "    for field in schema_fields:\n",
    "        if 'width' in field:\n",
    "            df[field['name']] = df[field['name']].str.zfill(field['width'])\n",
    "\n",
    "def validate_raw_strings(df, schema_fields):\n",
    "    \"\"\"Validate values in raw InfoGroup data according to string constraints.\n",
    "    Return list of dicts of invalid values.\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = {\n",
    "        'ZIP': {'number': True},\n",
    "        'ZIP4': {'number': True},\n",
    "        'COUNTY_CODE': {'number': True},\n",
    "        'AREA_CODE': {'number': True},\n",
    "        'PHONE': {'number': True},\n",
    "        'SIC': {'number': True},\n",
    "        'SIC0': {'number': True},\n",
    "        'SIC1': {'number': True},\n",
    "        'SIC2': {'number': True},\n",
    "        'SIC3': {'number': True},\n",
    "        'SIC4': {'number': True},\n",
    "        'NAICS': {'number': True},\n",
    "        'YEAR': {'notna': True, 'number': True},\n",
    "        'YP_CODE': {'number': True},\n",
    "        'EMPLOYEES': {'number': True},\n",
    "        'SALES': {'number': True},\n",
    "        'PARENT_EMPLOYEES': {'number': True},\n",
    "        'PARENT_SALES': {'number': True},\n",
    "        'YEAR_EST': {'number': True},\n",
    "        'ABI': {'unique': True, 'notna': True, 'number': True},\n",
    "        'SUBSIDIARY_NUMBER': {'number': True},\n",
    "        'PARENT_NUMBER': {'number': True},\n",
    "        'SITE_NUMBER': {'number': True},\n",
    "        'CENSUS_TRACT': {'number': True},\n",
    "        'CENSUS_BLOCK': {'number': True},\n",
    "        'LATITUDE': {'number': True},\n",
    "        'LONGITUDE': {'number': True},\n",
    "        'CBSA_CODE': {'number': True},\n",
    "        'CSA_CODE': {'number': True},\n",
    "        'FIPS_CODE': {'number': True}\n",
    "    }\n",
    "    \n",
    "    # the above hard coded list of constraints must be consistent with field availability in given year\n",
    "    constraints = {k: v for k, v in constraints.items() if k in df}\n",
    "    \n",
    "    \n",
    "    for field in schema_fields:\n",
    "        name = field['name']\n",
    "        if 'enum' in field:\n",
    "            if name not in constraints: constraints[name] = dict()\n",
    "            constraints[name]['cats'] = field['enum']\n",
    "        if 'width' in field:\n",
    "            constraints[name]['nchar'] = field['width']\n",
    "    \n",
    "    return util.validate_values(df, constraints)\n",
    "\n",
    "\n",
    "def convert_dtypes(df, schema_fields):\n",
    "    \"\"\"Inplace convert string columns to appropriate types.\"\"\"\n",
    "    \n",
    "    for col in ['YEAR', 'EMPLOYEES', 'SALES', 'PARENT_EMPLOYEES', 'PARENT_SALES', 'YEAR_EST', 'LATITUDE', 'LONGITUDE']:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "        \n",
    "    for field in schema_fields:\n",
    "        if 'enum' in field:\n",
    "            df[field['name']] = pd.Categorical(df[field['name']], categories=field['enum'])\n",
    "    \n",
    "\n",
    "def validate_raw_numbers(df):\n",
    "    \"\"\"Validate values in raw InfoGroup data according to numerical constraints.\n",
    "    Return list of dicts of invalid values.\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = {\n",
    "        'YEAR': {'eq': year},\n",
    "        'EMPLOYEES': {'ge': 0},\n",
    "        'SALES': {'ge': 0},\n",
    "        'PARENT_EMPLOYEES': {'ge': 0},\n",
    "        'PARENT_SALES': {'ge': 0},\n",
    "        'YEAR_EST': {'ge': 1000, 'le': year},\n",
    "        'LATITUDE': {'ge': 0, 'le': 90},\n",
    "        'LONGITUDE': {'ge': -180, 'le': 0}\n",
    "    }\n",
    "    return util.validate_values(df, constraints)\n",
    "\n",
    "def replace_invalid(df, invalid_list, replacement=np.nan):\n",
    "    \"\"\"Replace invalid values.\"\"\"\n",
    "    for inv in invalid_list:\n",
    "        df.loc[inv['idx'], inv['col']] = replacement\n",
    "        print(f'Replace invalid value `{inv[\"val\"]}` with `{replacement}` at .loc[{inv[\"idx\"]}, \\'{inv[\"col\"]}\\']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [1999, 2002, 2004, 2006]:\n",
    "    print(year)\n",
    "    sch = json.load(resources.get(f'infogroup/schema').path.open())\n",
    "    sch = [x for x in sch['fields'] if year in x['years']]\n",
    "\n",
    "    df = pd.read_csv(resources.get(f'infogroup/orig/{year}').path, dtype='str', encoding='latin-1')\n",
    "\n",
    "    df.rename(columns={x['original_name']: x['name'] for x in sch}, inplace=True)\n",
    "\n",
    "    if year == 2009:\n",
    "        pad_with_zeroes(df, sch)\n",
    "\n",
    "    invalid_str = validate_raw_strings(df, sch)\n",
    "    if len(invalid_str) < 100:\n",
    "        replace_invalid(df, invalid_str)\n",
    "    else:\n",
    "        print(invalid_str[:5])\n",
    "        raise Exception(f'Very many invalid_str values: {len(invalid_str)}')\n",
    "\n",
    "    convert_dtypes(df, sch)\n",
    "\n",
    "    invalid_num = validate_raw_numbers(df)\n",
    "    if len(invalid_num) < 100:\n",
    "        replace_invalid(df, invalid_num)\n",
    "    else:\n",
    "        print(f'Very many invalid_num values: {len(invalid_num)}')\n",
    "        print(invalid_num[:5])\n",
    "\n",
    "    df.to_csv(resources.get(f'infogroup/csv/{year}').path, index=False)\n",
    "    fastparquet.write(resources.get(f'infogroup/pq/{year}').path, df, write_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1997\n",
    "Replace invalid value `9 1` with `nan` at EMPLOYEES, 3756551.\n",
    "Replace invalid value `1 6` with `nan` at SALES, 2827962.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 2835233.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 5601711.\n",
    "Replace invalid value `2 930` with `nan` at SALES, 5601723.\n",
    "Replace invalid value `39 6 0` with `nan` at SALES, 5645704.\n",
    "Replace invalid value `26 0` with `nan` at SALES, 5645707.\n",
    "Replace invalid value `44 0` with `nan` at SALES, 5652929.\n",
    "Replace invalid value `A` with `nan` at SALES, 7949970.\n",
    "Replace invalid value `18 4` with `nan` at SALES, 10777263.\n",
    "Replace invalid value `/` with `nan` at PARENT_NUMBER, 1970582.\n",
    "\n",
    "1998\n",
    "Replace invalid value `F000` with `nan` at EMPLOYEES, 10554007.\n",
    "Replace invalid value `0` with `nan` at BUSINESS_STATUS, 10554007.\n",
    "Replace invalid value `0` with `nan` at OFFICE_SIZE_CODE, 10554007.\n",
    "Replace invalid value `IG4775007` with `nan` at ABI, 10554007.\n",
    "Replace invalid value `6` with `nan` at ADDRESS_TYPE, 10554007.\n",
    "\n",
    "2003\n",
    "Replace invalid value `1 6` with `nan` at SALES, 3179995.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 3188560.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 6401323.\n",
    "Replace invalid value `44 0` with `nan` at SALES, 6456105.\n",
    "Replace invalid value `26 0` with `nan` at SALES, 6516738.\n",
    "Replace invalid value `18 4` with `nan` at SALES, 12214748.\n",
    "\n",
    "2004\n",
    "Replace invalid value `1 6` with `nan` at SALES, 3132298.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 3140153.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 6330767.\n",
    "Replace invalid value `44 0` with `nan` at SALES, 6383666.\n",
    "Replace invalid value `26 0` with `nan` at SALES, 6441673.\n",
    "Replace invalid value `18 4` with `nan` at SALES, 12093651.\n",
    "\n",
    "2005\n",
    "Replace invalid value `1 6` with `nan` at SALES, 3262278.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 3270294.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 6602040.\n",
    "Replace invalid value `44 0` with `nan` at SALES, 6656877.\n",
    "Replace invalid value `26 0` with `nan` at SALES, 6716447.\n",
    "Replace invalid value `18 4` with `nan` at SALES, 12657797.\n",
    "\n",
    "2006\n",
    "Replace invalid value `1 6` with `nan` at SALES, 3348096.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 3356659.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 6729279.\n",
    "Replace invalid value `44 0` with `nan` at SALES, 6784843.\n",
    "Replace invalid value `26 0` with `nan` at SALES, 6848756.\n",
    "Replace invalid value `18 4` with `nan` at SALES, 12800833.\n",
    "\n",
    "2007\n",
    "Replace invalid value `1 6` with `nan` at SALES, 3445276.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 3452053.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 6929699.\n",
    "Replace invalid value `1493.0` with `nan` at YEAR_EST, 13571251.\n",
    "\n",
    "\n",
    "2008\n",
    "Replace invalid value `C` with `nan` at SALES, 2473525.\n",
    "Replace invalid value `1 6` with `nan` at SALES, 3527847.\n",
    "Replace invalid value `121  0` with `nan` at SALES, 3534969.\n",
    "Replace invalid value `1 6 0` with `nan` at SALES, 7040776.\n",
    "Replace invalid value `1493.0` with `nan` at YEAR_EST, 13754797. # replaced during numerical validation, constraint was year_est >= 1500\n",
    "\n",
    "2012\n",
    "Replace invalid value `INT` with `nan` at SALES, 12383573.\n",
    "\n",
    "2013\n",
    "Replace invalid value `INT` with `nan` at SALES, 13419903.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_df(year, cols=None):\n",
    "    \"\"\"Return one year of InfoGroup data with appropriate data types.\n",
    "    Subset of columns can be loaded by passing list to `cols`.\n",
    "    \"\"\"\n",
    "    res = resources.get(f'infogroup/pq/{year}')\n",
    "    return pd.read_parquet(res.path, 'fastparquet', columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
