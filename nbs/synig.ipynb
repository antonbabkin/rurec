{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynIG: Synthetic InfoGroup\n",
    "\n",
    "## Created columns\n",
    "\n",
    "- ABI: fake\n",
    "- COMPANY: fake\n",
    "- ADDRESS: fake\n",
    "- CITY: fake\n",
    "- ZIP: fake\n",
    "- COUNTY_CODE: sample\n",
    "- STATE: sample\n",
    "- FIPS_CODE: STATE + COUNTY_CODE\n",
    "- LATITUDE: sample + noise\n",
    "- LONGITUDE: sample + noise\n",
    "- YEAR: 1997-2017\n",
    "- NAICS: sample, 6 digits\n",
    "- EMPLOYEES: modelled\n",
    "- EMPLOYEES_CODE: derived from EMPLOYEES\n",
    "- SALES: derived from EMPLOYEES + noise\n",
    "- SALES_CODE: derived from SALES\n",
    "\n",
    "## Generation algorithm\n",
    "\n",
    "### Fake data\n",
    "\n",
    "Generated using [`mimesis`](https://github.com/lk-geimfari/mimesis) package for US locale.\n",
    "\n",
    "### Sample\n",
    "\n",
    "Sampled from InfoGroup separately for each state-sector, pooled across years.\n",
    "\n",
    "### Generated\n",
    "\n",
    "state space for each establishment:\n",
    "- size (employees)\n",
    "- age (automatic)\n",
    "- 2-digit NAICS (constant) \n",
    "- US state (constant)\n",
    "\n",
    "STATE x NAICS - some cells will be small, so use clustering to attach smallest cells. [Hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) maybe be a good approach. once data is generated, randomly assign each establishment from cluster to cells.\n",
    "\n",
    "initial state t0 (year 1997): fit cross-section distribution and draw desired N\n",
    "\n",
    "iteration for t:\n",
    "- draw exit probability, conditional on state at t-1\n",
    "- draw growth probability (because many do not grow)\n",
    "- conditional on growth, draw arc growth rate\n",
    "  - maybe fit [truncated normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.truncnorm.html) distribution\n",
    "  - since establishments with 1-5 employees constitute 75% of data, maybe fit some descrete growth distribution for them\n",
    "- compute $\\Delta$EMP from agr\n",
    "  - round up values < 1.\n",
    "- draw sample of births\n",
    "  - number equal to deaths: stable population\n",
    "  - birth rate conditional on year: dynamic population\n",
    "  - birth rate conditional on year and naics-state\n",
    "\n",
    "add other (random) variables: 6-digit naics, street address, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import shutil\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import mimesis\n",
    "import fastparquet\n",
    "\n",
    "from rurec import infogroup, resources\n",
    "# memory = joblib.Memory('../cache')\n",
    "memory = joblib.Memory(None) # disable caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NAICS sectors](https://www.census.gov/cgi-bin/sssd/naics/naicsrch?chart=2017)\n",
    "\n",
    "```\n",
    "Sector \tDescription\n",
    "11 \tAgriculture, Forestry, Fishing and Hunting\n",
    "21 \tMining, Quarrying, and Oil and Gas Extraction\n",
    "22 \tUtilities\n",
    "23 \tConstruction\n",
    "31-33 \tManufacturing\n",
    "42 \tWholesale Trade\n",
    "44-45 \tRetail Trade\n",
    "48-49 \tTransportation and Warehousing\n",
    "51 \tInformation\n",
    "52 \tFinance and Insurance\n",
    "53 \tReal Estate and Rental and Leasing\n",
    "54 \tProfessional, Scientific, and Technical Services\n",
    "55 \tManagement of Companies and Enterprises\n",
    "56 \tAdministrative and Support and Waste Management and Remediation Services\n",
    "61 \tEducational Services\n",
    "62 \tHealth Care and Social Assistance\n",
    "71 \tArts, Entertainment, and Recreation\n",
    "72 \tAccommodation and Food Services\n",
    "81 \tOther Services (except Public Administration)\n",
    "92 \tPublic Administration\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naics_sector(x):\n",
    "    \"\"\"Return 2-digit NAICS sector from full NAICS code.\n",
    "    Sectors with multiple codes are coded with just the first,\n",
    "    eg. \"31-33 Manufacturing\" is coded as \"31\".\n",
    "    \"\"\"\n",
    "    join_multiple = {'32': '31', '33': '31', '45': '44', '49': '48'}\n",
    "    return x.str[:2].replace(join_multiple)\n",
    "\n",
    "naics_sectors = ['11', '21', '22', '23', '31', '42', '44', '48', '51', '52', '53', '54', '55', '56', '61', '62', '71', '72', '81', '92']\n",
    "\n",
    "state_fips_code = {\n",
    "    'AL':'01',\n",
    "    'AK':'02',\n",
    "    'AS':'60',\n",
    "    'AZ':'04',\n",
    "    'AR':'05',\n",
    "    'CA':'06',\n",
    "    'CO':'08',\n",
    "    'CT':'09',\n",
    "    'DE':'10',\n",
    "    'DC':'11',\n",
    "    'FL':'12',\n",
    "    'FM':'64',\n",
    "    'GA':'13',\n",
    "    'GU':'66',\n",
    "    'HI':'15',\n",
    "    'ID':'16',\n",
    "    'IL':'17',\n",
    "    'IN':'18',\n",
    "    'IA':'19',\n",
    "    'KS':'20',\n",
    "    'KY':'21',\n",
    "    'LA':'22',\n",
    "    'ME':'23',\n",
    "    'MH':'68',\n",
    "    'MD':'24',\n",
    "    'MA':'25',\n",
    "    'MI':'26',\n",
    "    'MN':'27',\n",
    "    'MS':'28',\n",
    "    'MO':'29',\n",
    "    'MT':'30',\n",
    "    'NE':'31',\n",
    "    'NV':'32',\n",
    "    'NH':'33',\n",
    "    'NJ':'34',\n",
    "    'NM':'35',\n",
    "    'NY':'36',\n",
    "    'NC':'37',\n",
    "    'ND':'38',\n",
    "    'MP':'69',\n",
    "    'OH':'39',\n",
    "    'OK':'40',\n",
    "    'OR':'41',\n",
    "    'PW':'70',\n",
    "    'PA':'42',\n",
    "    'PR':'72',\n",
    "    'RI':'44',\n",
    "    'SC':'45',\n",
    "    'SD':'46',\n",
    "    'TN':'47',\n",
    "    'TX':'48',\n",
    "    'UM':'74',\n",
    "    'UT':'49',\n",
    "    'VT':'50',\n",
    "    'VA':'51',\n",
    "    'VI':'78',\n",
    "    'WA':'53',\n",
    "    'WV':'54',\n",
    "    'WI':'55',\n",
    "    'WY':'56'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-sector distribution\n",
    "\n",
    "Supress cells with less than 1000 establishments per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for state in state_fips_code.keys():\n",
    "    df = infogroup.get_df(cols=['NAICS'], states=[state])\n",
    "    df['SECTOR'] = naics_sector(df['NAICS'])\n",
    "    counts[state] = df.groupby('SECTOR').size()\n",
    "df = pd.concat(counts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_years = 21\n",
    "min_est = 1000\n",
    "s = df.stack().dropna()\n",
    "s = s[s > n_years * min_est]\n",
    "state_sector_weights = s / s.sum()\n",
    "joblib.dump(state_sector_weights, resources.paths.root / 'tmp/state_sector_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sector_weights = joblib.load(resources.paths.root / 'tmp/state_sector_weights')\n",
    "state_sector_weights.index = state_sector_weights.index.swaplevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fake(size):\n",
    "    faker = mimesis.Generic('en')\n",
    "    data = []\n",
    "    for _ in range(size):\n",
    "        company = faker.business.company().upper()\n",
    "        address = faker.address.address().upper()\n",
    "        city = faker.address.city().upper()\n",
    "        zip = faker.address.zip_code()\n",
    "        data.append((company, address, city, zip))\n",
    "    return pd.DataFrame(data, columns=['COMPANY', 'ADDRESS', 'CITY', 'ZIP'])\n",
    "\n",
    "gen_fake(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_sampling_source(state, sector):\n",
    "    df = infogroup.get_df(cols=['ABI', 'NAICS', 'COUNTY_CODE', 'LATITUDE', 'LONGITUDE'], states=[state])\n",
    "    del df['YEAR']\n",
    "    df = df[naics_sector(df['NAICS']) == sector]\n",
    "    df = df.drop_duplicates('ABI').dropna()\n",
    "    del df['ABI']\n",
    "    df['NAICS'] = df['NAICS'].str[:6]\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def gen_sample(state, sector, size):\n",
    "    src = get_sampling_source(state, sector)\n",
    "    df = src[['COUNTY_CODE', 'LATITUDE', 'LONGITUDE']].sample(size, replace=True).reset_index(drop=True)\n",
    "    df['NAICS'] = src['NAICS'].sample(size, replace=True).reset_index(drop=True)\n",
    "    df['LATITUDE'] += sp.stats.uniform.rvs(-0.5, 1, size=size)\n",
    "    df['LONGITUDE'] += sp.stats.uniform.rvs(-0.5, 1, size=size)\n",
    "    df['STATE'] = state\n",
    "    df['FIPS_CODE'] = df['STATE'].replace(state_fips_code) + df['COUNTY_CODE']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_sample('WI', '23', 1000)\n",
    "plt.scatter('LONGITUDE', 'LATITUDE', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_model_src(state, sector):\n",
    "    \n",
    "    data_years = range(1997, 2018)\n",
    "\n",
    "    df = infogroup.get_df(cols=['ABI', 'YEAR', 'NAICS', 'EMPLOYEES'], states=[state])\n",
    "    df = df.loc[naics_sector(df['NAICS']) == sector, ['YEAR', 'ABI', 'EMPLOYEES']]\n",
    "                \n",
    "    df = df.sort_values(['ABI', 'YEAR'])\n",
    "    df['EMPLOYEES_LAG'] = df.groupby('ABI')['EMPLOYEES'].shift()\n",
    "    df['YEAR_LAG'] = df.groupby('ABI')['YEAR'].shift()\n",
    "    df['YEAR_LEAD'] = df.groupby('ABI')['YEAR'].shift(-1)\n",
    "\n",
    "    df['BIRTH'] = df['YEAR_LAG'].isna()\n",
    "    df.loc[df['YEAR'] == data_years[0], 'BIRTH'] = np.nan\n",
    "    df['DEATH'] = df['YEAR_LEAD'].isna()\n",
    "    df.loc[df['YEAR'] == data_years[-1], 'DEATH'] = np.nan\n",
    "\n",
    "    \n",
    "    return df[['EMPLOYEES', 'EMPLOYEES_LAG', 'BIRTH', 'DEATH']].reset_index(drop=True)\n",
    "\n",
    "get_model_src('WI', '23').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Birth:\n",
    "    \"\"\"Generate random series of given `size` that resembles distribution of 1-d array `x`.\n",
    "    \"\"\"\n",
    "    def fit(self, x):\n",
    "        x = np.array(x)\n",
    "        x = x[(0 < x) & (x < 100)]\n",
    "        x = np.log10(x)\n",
    "        b = np.unique(np.quantile(x, np.linspace(0, 1, 50)))\n",
    "        self.rv = sp.stats.rv_histogram(np.histogram(x, b))()\n",
    "    def predict(self, size=1):\n",
    "        return (10**self.rv.rvs(size)).round()\n",
    "\n",
    "d = sp.stats.lognorm.rvs(1.5, size=10_000)\n",
    "d = d[d < 150]\n",
    "m = Birth()\n",
    "m.fit(d)\n",
    "dh = m.predict(len(d))\n",
    "\n",
    "plt.hist([d, dh])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Death:\n",
    "    \"\"\"Generate random series of death indicator, predicted from 1-d array `x`.\n",
    "    Death probability for Bernoulli draws is estimated in each bin formed by `breaks`.\n",
    "    \"\"\"\n",
    "    def __init__(self, breaks=None):\n",
    "        if breaks is None:\n",
    "            breaks = [0, np.inf]\n",
    "        self.bins = pd.IntervalIndex.from_breaks(breaks)\n",
    "    def fit(self, x, y):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        d = pd.DataFrame({'x': x, 'y': y, 'c': pd.cut(x, self.bins)})\n",
    "        p = d.groupby(['c', 'y']).size().unstack()\n",
    "        self.prob = (p[1] / p.sum(1)).fillna(0)\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        d = pd.DataFrame({'x': x, 'c': pd.cut(x, self.bins)}).sort_values('c')\n",
    "        counts = d['c'].value_counts().sort_index()\n",
    "        y = []\n",
    "        for b, size in counts.iteritems():\n",
    "            y.append(sp.stats.bernoulli.rvs(self.prob[b], size=size))\n",
    "        d['y'] = np.concatenate(y)\n",
    "        return d['y'].sort_index().values\n",
    "            \n",
    "        \n",
    "\n",
    "d = pd.DataFrame({'x': [1,1,1,1,1,6,7,8] * 1000,\n",
    "                  'y': [0,0,1,1,1,0,0,1] * 1000})\n",
    "m = Death()\n",
    "m.fit(d['x'], d['y'])\n",
    "assert (m.prob == [0.5]).all()\n",
    "assert np.isclose(m.predict(sp.stats.randint.rvs(1, 10, size=1000)).mean(), 0.5, atol=0.03)\n",
    "\n",
    "m = Death([0, 1, 5, 10])\n",
    "m.fit(d['x'], d['y'])\n",
    "assert (m.prob == [3/5, 0, 1/3]).all()\n",
    "d['c'] = pd.cut(d['x'], m.bins)\n",
    "d['yh'] = m.predict(d['x'])\n",
    "t = d.groupby('c')[['y', 'yh']].mean()\n",
    "assert np.allclose(t['y'], t['yh'], atol=0.03, equal_nan=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arc growth rate: $g = \\frac{x_1 - x_0}{0.5x_0 + 0.5x_1}$\n",
    "\n",
    "$x_1 = x_0\\frac{2+g}{2-g}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Growth:\n",
    "    \"\"\"Generate random values for next period.\"\"\"\n",
    "    def fit(self, x0, x1):\n",
    "        x0 = np.array(x0)\n",
    "        x1 = np.array(x1)\n",
    "        gr_mask = (x0 != x1)\n",
    "        self.gr_prob = gr_mask.mean()\n",
    "        x0 = x0[gr_mask]\n",
    "        x1 = x1[gr_mask]\n",
    "        gr = (x1 - x0) * 2 / (x0 + x1)\n",
    "        \n",
    "        a, b = -1.5, 1.5\n",
    "        rv_par = sp.stats.truncnorm.fit(gr[(a < gr) & (gr < b)], fix_a=a, fix_b=b)\n",
    "        self.rv = sp.stats.truncnorm(*rv_par)\n",
    "        \n",
    "    def predict(self, x0):\n",
    "        x0 = np.array(x0)\n",
    "        d = pd.DataFrame({'x0': x0})\n",
    "        d['growth'] = sp.stats.bernoulli.rvs(self.gr_prob, size=len(x0)).astype('bool')\n",
    "        d['x1'] = np.nan\n",
    "        d.loc[~d['growth'], 'x1'] = d['x0']\n",
    "        gr = self.rv.rvs(d['growth'].sum())\n",
    "        d.loc[d['growth'], 'x1'] = d.loc[d['growth'], 'x0'] * (2 + gr) / (2 - gr)\n",
    "        d['x1'] = np.ceil(d['x1'])\n",
    "        d.loc[d['growth'] & (d['x1'] == d['x0']) & (d['x1'] > 1), 'x1'] -= 1\n",
    "        return d['x1'].values\n",
    "\n",
    "n = 10_000\n",
    "gr_prob = 0.2\n",
    "x0 = sp.stats.randint.rvs(1, 20, size=n)\n",
    "x1 = sp.stats.randint.rvs(1, 20, size=n)\n",
    "gr_mask = sp.stats.bernoulli.rvs(gr_prob, size=n) == 0\n",
    "x1[gr_mask] = x0[gr_mask] \n",
    "g = Growth()\n",
    "g.fit(x0, x1)\n",
    "xh = g.predict(x0)\n",
    "plt.hist([x1-x0, xh-x0], bins=range(-20, 21))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def gen_model_data(state, sector, size=None):\n",
    "    syn_years = range(2001, 2021)\n",
    "\n",
    "    df = get_model_src(state, sector)\n",
    "    \n",
    "    if size is None:\n",
    "        size = len(df)\n",
    "    size_per_year = size // len(syn_years)\n",
    "\n",
    "    init = Birth()\n",
    "    init.fit(df['EMPLOYEES'].dropna())\n",
    "\n",
    "    birth = Birth()\n",
    "    birth.fit(df.loc[df['BIRTH'] == 1, 'EMPLOYEES'].dropna())\n",
    "\n",
    "    death = Death([0, 1, 5, 20, 50, np.inf])\n",
    "    d = df[['EMPLOYEES', 'DEATH']].dropna()\n",
    "    death.fit(d['EMPLOYEES'], d['DEATH'])\n",
    "\n",
    "    growth = Growth()\n",
    "    d = df[['EMPLOYEES', 'EMPLOYEES_LAG']].dropna()\n",
    "    growth.fit(d['EMPLOYEES_LAG'], d['EMPLOYEES'])\n",
    "\n",
    "    df = []\n",
    "\n",
    "    next_est_id = 1\n",
    "    emp0 = init.predict(size_per_year)\n",
    "    df0 = pd.DataFrame({'ABI': range(next_est_id, size_per_year + 1), 'EMPLOYEES': emp0})\n",
    "    next_est_id = size_per_year + 1\n",
    "    df0['DEATH'] = death.predict(emp0)\n",
    "    df0['YEAR'] = syn_years[0]\n",
    "    df.append(df0)\n",
    "\n",
    "    for year in syn_years[1:]:\n",
    "        emp1_cont = growth.predict(df0.loc[df0['DEATH'] == 0, 'EMPLOYEES'])\n",
    "        death_count = (df0['DEATH'] == 1).sum()\n",
    "        emp1_bir = birth.predict(death_count)\n",
    "        est_id1 = df0.loc[df0['DEATH'] == 0, 'ABI'].append(pd.Series(range(next_est_id, next_est_id + death_count)))\n",
    "        next_est_id = next_est_id + death_count\n",
    "        df1 = pd.DataFrame({'ABI': est_id1, 'EMPLOYEES': np.concatenate([emp1_cont, emp1_bir])})\n",
    "        df1['DEATH'] = death.predict(df1['EMPLOYEES'])\n",
    "        df1['YEAR'] = year\n",
    "        df.append(df1)\n",
    "        df0 = df1\n",
    "\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "gen_model_data('WI', '23').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_chunk(state, sector, size):\n",
    "    df = gen_model_data(state, sector, size)\n",
    "    ids = df['ABI'].unique()\n",
    "    dfc = gen_sample(state, sector, len(ids))\n",
    "    dfc = pd.concat([dfc, gen_fake(len(ids))], 1)\n",
    "    dfc['ABI'] = ids\n",
    "    df = df.merge(dfc, 'left', 'ABI')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=resources.paths.root / 'logs/synth.log',filemode='w', level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_per_year = 1_000_000\n",
    "n_years = 20\n",
    "total_size = size_per_year * n_years\n",
    "\n",
    "pq_path = resources.paths.root / 'data/synig.pq'\n",
    "if pq_path.exists():\n",
    "    shutil.rmtree(pq_path)\n",
    "pq_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_paths = []\n",
    "for state, sector in state_sector_weights.index:\n",
    "    logging.info(f'{state}, {sector}')\n",
    "    p = pq_path / f'STATE={state}/SECTOR={sector}'\n",
    "    if not p.exists():\n",
    "        w = state_sector_weights[(state, sector)]\n",
    "        df = gen_chunk(state, sector, int(w * total_size))\n",
    "        del df['STATE']\n",
    "        fastparquet.write(str(p), df, file_scheme='hive', write_index=False, partition_on=['YEAR'])\n",
    "    partition_paths.append(str(p))\n",
    "fastparquet.writer.merge(partition_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = resources.paths.root / 'data/synig'\n",
    "if csv_path.exists(): shutil.rmtree(csv_path)\n",
    "csv_path.mkdir()\n",
    "pf = fastparquet.ParquetFile(str(pq_path))\n",
    "for year in range(2001, 2021):\n",
    "    df = pf.to_pandas(filters=[('YEAR', '==', year)])\n",
    "    df['ABI'] = df['ABI'].astype(str).str.zfill(9)\n",
    "    df['NAICS'] += '00'\n",
    "    df.loc[df.sample(frac=0.06).index, 'EMPLOYEES'] = np.nan\n",
    "    df['EMPLOYEES'] = df['EMPLOYEES'].astype('Int32')\n",
    "    df = df[['YEAR', 'STATE', 'SECTOR', 'ABI', 'NAICS', 'EMPLOYEES', 'COMPANY', 'ADDRESS', 'CITY', 'ZIP', 'COUNTY_CODE', 'FIPS_CODE', 'LONGITUDE', 'LATITUDE']]\n",
    "    df.to_csv(csv_path / f'{year}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
