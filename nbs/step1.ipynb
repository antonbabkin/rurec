{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Company', 'Address Line 1', 'City', 'State', 'ZipCode', 'Zip4',\n",
    "       'County Code', 'Area Code', 'IDCode', 'Location Employee Size Code',\n",
    "       'Location Sales Volume Code', 'Primary SIC Code', 'SIC6_Descriptions',\n",
    "       'Primary NAICS Code', 'NAICS8 Descriptions', 'SIC Code',\n",
    "       'SIC6_Descriptions (SIC)', 'SIC Code 1', 'SIC6_Descriptions (SIC1)',\n",
    "       'SIC Code 2', 'SIC6_Descriptions(SIC2)', 'SIC Code 3',\n",
    "       'SIC6_Descriptions(SIC3)', 'SIC Code 4', 'SIC6_Descriptions(SIC4)',\n",
    "       'Archive Version Year', 'Yellow Page Code',\n",
    "       'Employee Size (5) - Location', 'Sales Volume (9) - Location',\n",
    "       'Business Status Code', 'Industry Specific First Byte',\n",
    "       'Year Established', 'Office Size Code', 'Company Holding Status', 'ABI',\n",
    "       'Subsidiary Number', 'Parent Number', 'Parent Actual Employee Size',\n",
    "       'Parent Actual Sales Volume', 'Parent Employee Size Code',\n",
    "       'Parent Sales Volume Code', 'Site Number', 'Address Type Indicator',\n",
    "       'Population Code', 'Census Tract', 'Census Block', 'Latitude',\n",
    "       'Longitude', 'Match Code', 'CBSA Code', 'CBSA Level', 'CSA Code',\n",
    "       'FIPS Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the FIPS state code to each record. Prior examination shows some data entry \n",
    "# errors in the FIPS Code variable so we do this by brute force.\n",
    "state_fips = {\n",
    "'AL':'01',\n",
    "'AK':'02',\n",
    "'AS':'60',\n",
    "'AZ':'04',\n",
    "'AR':'05',\n",
    "'CA':'06',\n",
    "'CO':'08',\n",
    "'CT':'09',\n",
    "'DE':'10',\n",
    "'DC':'11',\n",
    "'FL':'12',\n",
    "'FM':'64',\n",
    "'GA':'13',\n",
    "'GU':'66',\n",
    "'HI':'15',\n",
    "'ID':'16',\n",
    "'IL':'17',\n",
    "'IN':'18',\n",
    "'IA':'19',\n",
    "'KS':'20',\n",
    "'KY':'21',\n",
    "'LA':'22',\n",
    "'ME':'23',\n",
    "'MH':'68',\n",
    "'MD':'24',\n",
    "'MA':'25',\n",
    "'MI':'26',\n",
    "'MN':'27',\n",
    "'MS':'28',\n",
    "'MO':'29',\n",
    "'MT':'30',\n",
    "'NE':'31',\n",
    "'NV':'32',\n",
    "'NH':'33',\n",
    "'NJ':'34',\n",
    "'NM':'35',\n",
    "'NY':'36',\n",
    "'NC':'37',\n",
    "'ND':'38',\n",
    "'MP':'69',\n",
    "'OH':'39',\n",
    "'OK':'40',\n",
    "'OR':'41',\n",
    "'PW':'70',\n",
    "'PA':'42',\n",
    "'PR':'72',\n",
    "'RI':'44',\n",
    "'SC':'45',\n",
    "'SD':'46',\n",
    "'TN':'47',\n",
    "'TX':'48',\n",
    "'UM':'74',\n",
    "'UT':'49',\n",
    "'VT':'50',\n",
    "'VA':'51',\n",
    "'VI':'78',\n",
    "'WA':'53',\n",
    "'WV':'54',\n",
    "'WI':'55',\n",
    "'WY':'56'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_correct(yr):\n",
    "    print(f'\\n{yr}:')\n",
    "    print(f'\\n{yr}:',file=logfile)\n",
    "    dir = '/InfoGroup/data/original/'\n",
    "    xdir = '/tmp/xtrcts/'\n",
    "    fname = f'{yr}_Business_Academic_QCQ_utf-8'\n",
    "    # Extract the annual file from the zip archive\n",
    "    with ZipFile(f'{dir}{fname}.zip','r') as myzip:\n",
    "        myzip.extract(f'{fname}.csv',f'{xdir}')\n",
    "        df = pd.read_csv(f'{xdir}{fname}.csv',low_memory=False,usecols=cols,dtype=object)\n",
    "    \n",
    "    # Delete the temp file\n",
    "    os.remove(f'{xdir}{fname}.csv')  \n",
    "    \n",
    "    # Prepare CBSA Code and CBSA Level for correction.\n",
    "    df['CBSA Level'].fillna('0',inplace=True)\n",
    "    df['CBSA Code'].fillna('00000',inplace=True)\n",
    "    df[['CBSA Code','CBSA Level']] = df[['CBSA Code','CBSA Level']].astype(int,copy=False)\n",
    "\n",
    "    # Correct and overwrite the state FIPS code.\n",
    "    df['FIPS Code'] = df.apply(new_fips, axis=1)\n",
    "    \n",
    "    # zero-fill the ZipCode value\n",
    "    df['ZipCode'] = df['ZipCode'].apply(lambda x: x.zfill(5) if len(x) < 5 == 0 else x)\n",
    "    return df\n",
    "\n",
    "def CBSA_partition(df):    \n",
    "    urban = df[(df['CBSA Level'] > 0)]\n",
    "    rural = df[(df['CBSA Code'] > 0) & (df['CBSA Level']==0)]\n",
    "    unknown = df[(df['CBSA Code'] < 0) | ((df['CBSA Code'] == 0) & (df['CBSA Level']==0))]\n",
    "    unknown['FIPS Code'] = unknown['FIPS Code'].astype(int)\n",
    "    \n",
    "    nrows = len(df)\n",
    "    sum_of_parts = len(urban) + len(rural) + len(unknown)\n",
    "    if sum_of_parts != nrows:\n",
    "        print(f'Error in dividing enterprises into categories:',file=logfile)\n",
    "        print(f'\\t{nrows} != {sum_of_parts}',file=logfile)\n",
    "        \n",
    "    corrected = extract_corrections(unknown)\n",
    "    corrected.drop(columns=['CBSA','FIPS Code_r','LSAD'],inplace=True)\n",
    "    corrected.rename(columns={\"FIPS Code_l\": \"FIPS Code\"},inplace=True)\n",
    "\n",
    "    print(corrected['CBSA Level'].value_counts(),file=logfile)\n",
    "    return (urban, rural, corrected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_code(state):\n",
    "    \"\"\" Establish an accurate state FIPS code\"\"\"\n",
    "    return state_fips[state]\n",
    "\n",
    "def new_fips(row):\n",
    "    if str(row['FIPS Code']) == 'nan' or str(row['County Code']) == 'nan':\n",
    "        return '00000'\n",
    "    else:\n",
    "        return state_fips[row['State']] + str(row['County Code'])\n",
    "\n",
    "def extract_corrections(unknowns):\n",
    "    \"\"\"Extracts CBSA Code and appropriate CBSA Level for a list of InfoGroup FIPS Codes\"\"\"\n",
    "    # Files cross-references CBSA codes and county FIPS codes.\n",
    "    # Variable 'STCOU' is the 5-digit county FIPS code. The CBSA Level is inferred from the\n",
    "    # text in the 'LSAD' variable.\n",
    "    unk = unknowns.join(cbsa_df,on='FIPS Code',how='left',lsuffix='_l',rsuffix='_r')\n",
    "    unk['CBSA Level'] = 0\n",
    "    for i in unk.index:\n",
    "        if str(unk.at[i,'LSAD']).find(\"Metropolitan\") > -1:\n",
    "            unk.at[i,'CBSA Level'] = 2\n",
    "        elif str(unk.at[i,'LSAD']).find(\"Micropolitan\") > -1:\n",
    "            unk.at[i,'CBSA Level'] = 1    \n",
    "    return unk\n",
    "    \n",
    "def showtime(num):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(str(num),'  ',dt_string)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference datasets and derived data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census relationship files: CBSAs to counties\n",
    "cbsa_df = pd.read_csv(f'/home/tflory/notebooks/InfoGroup/rurality/Relationship_Files/cbsa-county-relationships-2017.csv',\n",
    "                      usecols=['STCOU','CBSA','LSAD']).fillna(-9)\n",
    "cbsa_df.rename(columns={'STCOU':'FIPS Code'},inplace=True)\n",
    "cbsa_df['FIPS Code'] = cbsa_df['FIPS Code'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a log file.\n",
    "logfile = open('/InfoGroup/data/rurality/logs/step1.log','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in range(1997,2018):\n",
    "    showtime('start')\n",
    "    df = extract_and_correct(yr)\n",
    "    showtime('extract_and_correct')\n",
    "    (urban,rural,corrected) = CBSA_partition(df)\n",
    "    showtime('CBSA_partition')\n",
    "    final_df = pd.concat([urban,rural,corrected],ignore_index=True)\n",
    "    showtime('inline concat')\n",
    "    final_df.to_csv(f'/InfoGroup/data/rurality/step1_{yr}.csv',index=None)\n",
    "    showtime('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
